{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad47c039",
   "metadata": {},
   "source": [
    "# 02 - Preprocesamiento y Feature Engineering\n",
    "\n",
    "**Objetivo:** En este notebook, se prepara el dataset para el entrenamiento de modelos. Se aplican las decisiones tomadas en el EDA, se seleccionan las características más importantes, se dividen los datos y se escalan para dejarlos en un formato óptimo para los algoritmos de Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a029fb2",
   "metadata": {},
   "source": [
    "## 1. Carga de Datos e Importaciones\n",
    "Importamos las librerías necesarias y cargamos el dataset limpio y optimizado que generamos en el primer notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43a23240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8f9a631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset optimizado\n",
    "df = pd.read_parquet(r'../data/processed/cic_ids_2017_optimized.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010a59d6",
   "metadata": {},
   "source": [
    "## 2. Selección Manual de Características\n",
    "Basado en la alta correlación (0.89) encontrada en el EDA entre `Flow Duration` y `Flow IAT Mean`, eliminamos esta última para reducir la redundancia en el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cab7a80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección Manual (Basada en Correlación)\n",
    "# En el EDA, vimos una correlación muy alta (0.89) entre 'Flow Duration' y 'Flow IAT Mean'.\n",
    "# Decidimos eliminar 'Flow IAT Mean' para reducir la redundancia.\n",
    "df.drop(columns = ['Flow IAT Mean'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da8bdd6",
   "metadata": {},
   "source": [
    "## 3. División en Características (X) y Objetivo (y)\n",
    "Separamos nuestro DataFrame en dos: `X` contendrá todas las características predictoras y `y` contendrá la columna objetivo que queremos predecir (`Label`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bfb8d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Label', axis = 1)\n",
    "y = df['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40365e72",
   "metadata": {},
   "source": [
    "## 4. División en Conjuntos de Entrenamiento y Prueba (Train/Test Split)\n",
    "Dividimos los datos en un conjunto de entrenamiento (70%) y uno de prueba (30%). Es crucial hacer esto **antes** de aplicar técnicas como la selección automática o el escalado para evitar la fuga de datos (data leakage).\n",
    "\n",
    "Se utiliza `stratify = y` para asegurar que la proporción de clases (el desbalance) sea la misma tanto en el conjunto de entrenamiento como en el de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2eb65cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb40cb2",
   "metadata": {},
   "source": [
    "## 5. Selección Automática de Características\n",
    "Utilizamos un modelo `RandomForestClassifier` como \"juez\" a través de `SelectFromModel` para evaluar la importancia de las características restantes. Nos quedamos con aquellas cuya importancia está por encima de la mediana, reduciendo así la dimensionalidad y enfocándonos en las variables más informativas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0040f329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Proceso de selección completado.\n",
      "Número original de características: 77\n",
      "Número de características seleccionadas: 39\n",
      "\n",
      "Características seleccionadas:\n",
      "['Destination Port', 'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets', 'Total Length of Fwd Packets', 'Total Length of Bwd Packets', 'Fwd Packet Length Max', 'Fwd Packet Length Mean', 'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Max', 'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd Header Length', 'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s', 'Max Packet Length', 'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance', 'PSH Flag Count', 'Average Packet Size', 'Avg Fwd Segment Size', 'Avg Bwd Segment Size', 'Fwd Header Length.1', 'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Bytes', 'Init_Win_bytes_forward', 'Init_Win_bytes_backward', 'act_data_pkt_fwd', 'min_seg_size_forward']\n"
     ]
    }
   ],
   "source": [
    "# Creamos el modelo que actuará como selector\n",
    "selector_model = RandomForestClassifier(n_estimators = 50, random_state = 42, n_jobs= -1)\n",
    "\n",
    "# Creamos el objeto selector, que elegirá las características con importancia > a la mediana\n",
    "selector = SelectFromModel(estimator = selector_model, threshold = 'median')\n",
    "\n",
    "# Entrenamos el selector\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# Obtenemos los nombres de las columnas seleccionadas\n",
    "selected_features_mask = selector.get_support()\n",
    "selected_features_names = X_train.columns[selected_features_mask]\n",
    "\n",
    "# Transformamos nuestros conjuntos para quedarnos solo con las columnas seleccionadas\n",
    "X_train_selected = selector.transform(X_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# Reportamos los resultados\n",
    "print(\"\\nProceso de selección completado.\")\n",
    "print(\"Número original de características:\", X_train.shape[1])\n",
    "print(\"Número de características seleccionadas:\", X_train_selected.shape[1])\n",
    "print(\"\\nCaracterísticas seleccionadas:\")\n",
    "print(selected_features_names.tolist())\n",
    "\n",
    "# Convertimos los arrays de numpy de vuelta a DataFrames de Pandas\n",
    "X_train_selected = pd.DataFrame(X_train_selected, columns = selected_features_names)\n",
    "X_test_selected = pd.DataFrame(X_test_selected, columns = selected_features_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e43255d",
   "metadata": {},
   "source": [
    "## 6. Escalado de Características Numéricas\n",
    "Aplicamos un `StandardScaler` para estandarizar las características numéricas (media 0, desviación estándar 1). El escalador se \"ajusta\" (`.fit()`) **únicamente** con los datos de entrenamiento y luego se aplica (`.transform()`) a ambos conjuntos (entrenamiento y prueba) para mantener la consistencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "207d1096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el escalador\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# \"Ajustamos\" el escalador SÓLO con los datos de entrenamiento para que aprenda la media y desviación estándar\n",
    "scaler.fit(X_train_selected)\n",
    "\n",
    "# \"Transformamos\" tanto el conjunto de entrenamiento como el de prueba\n",
    "X_train_scaled = scaler.transform(X_train_selected)\n",
    "X_test_scaled = scaler.transform(X_test_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d775fdba",
   "metadata": {},
   "source": [
    "## 7. Guardado de los Datos Preprocesados\n",
    "\n",
    "Finalmente, guardamos todos los conjuntos de datos (`X_train_scaled`, `X_test_scaled`, `y_train`, `y_test`) y la lista de características seleccionadas en un único archivo `.joblib`.\n",
    "\n",
    "Esto nos permitirá cargar estos datos ya listos y procesados directamente en el siguiente notebook (`03_model_training`), sin necesidad de volver a ejecutar todo este pipeline de preprocesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eed357fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/processed/cicids2017_preprocessed_data.joblib']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos un diccionario para guardar todos nuestros datos de forma ordenada\n",
    "datos_preprocesados = {\n",
    "    'X_train_scaled': X_train_scaled,\n",
    "    'X_test_scaled': X_test_scaled,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test,\n",
    "    'selected_features': selected_features_names # Guardamos también los nombres de las columnas\n",
    "}\n",
    "\n",
    "# Definimos la ruta de guardado\n",
    "ruta_guardado = r'../data/processed/cicids2017_preprocessed_data.joblib'\n",
    "\n",
    "# Guardamos el diccionario en un único archivo\n",
    "dump(datos_preprocesados, ruta_guardado)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Cyber-Project)",
   "language": "python",
   "name": "cyber-project-py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
