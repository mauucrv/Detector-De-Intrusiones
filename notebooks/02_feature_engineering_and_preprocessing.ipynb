{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad47c039",
   "metadata": {},
   "source": [
    "# 02 - Preprocesamiento y Feature Engineering\n",
    "\n",
    "**Objetivo:** En este notebook, preparamos el dataset para el entrenamiento de modelos de Machine Learning. Las tareas incluyen: selección de características basada en el EDA, separación de datos en conjuntos de entrenamiento y prueba, y escalado de las características numéricas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a029fb2",
   "metadata": {},
   "source": [
    "## 1. Carga de Datos e Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43a23240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8f9a631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset optimizado\n",
    "df = pd.read_parquet(r'../data/processed/cic_ids_2017_optimized.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010a59d6",
   "metadata": {},
   "source": [
    "## 2. Selección de Características (Feature Selection)\n",
    "\n",
    "Basado en el Análisis Exploratorio de Datos (EDA), tomaremos decisiones sobre qué características conservar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cab7a80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección Manual (Basada en Correlación)\n",
    "# En el EDA, vimos una correlación muy alta (0.89) entre 'Flow Duration' y 'Flow IAT Mean'.\n",
    "# Decidimos eliminar 'Flow IAT Mean' para reducir la redundancia.\n",
    "df.drop(columns = ['Flow IAT Mean'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da8bdd6",
   "metadata": {},
   "source": [
    "## 3. Separación de Datos\n",
    "\n",
    "Separamos nuestras variables predictoras (**X**) de nuestra variable objetivo (**y**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bfb8d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Label', axis = 1)\n",
    "y = df['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40365e72",
   "metadata": {},
   "source": [
    "## 4. División en Conjuntos de Entrenamiento y Prueba (Train/Test Split)\n",
    "\n",
    "Dividimos el dataset. Es **crucial** hacer esto **antes** de cualquier otro preprocesamiento (como el escalado) para evitar la fuga de datos (data leakage). Usamos **stratify = y** para asegurar que la proporción de clases sea la misma en ambos conjuntos, lo cual es vital para datasets desbalanceados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2eb65cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb40cb2",
   "metadata": {},
   "source": [
    "## 5. Selección de Características Automática (SelectFromModel)\n",
    "\n",
    "Ahora usamos un modelo **RandomForest** como \"juez\" para que nos ayude a seleccionar las características más importantes de forma automática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0040f329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Proceso de selección completado.\n",
      "Número original de características: 77\n",
      "Número de características seleccionadas: 39\n",
      "\n",
      "Características seleccionadas:\n",
      "['Destination Port', 'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets', 'Total Length of Fwd Packets', 'Total Length of Bwd Packets', 'Fwd Packet Length Max', 'Fwd Packet Length Mean', 'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Max', 'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd Header Length', 'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s', 'Max Packet Length', 'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance', 'PSH Flag Count', 'Average Packet Size', 'Avg Fwd Segment Size', 'Avg Bwd Segment Size', 'Fwd Header Length.1', 'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Bytes', 'Init_Win_bytes_forward', 'Init_Win_bytes_backward', 'act_data_pkt_fwd', 'min_seg_size_forward']\n"
     ]
    }
   ],
   "source": [
    "# Creamos el modelo que actuará como selector\n",
    "selector_model = RandomForestClassifier(n_estimators = 50, random_state = 42, n_jobs= -1)\n",
    "\n",
    "# Creamos el objeto selector, que elegirá las características con importancia > a la mediana\n",
    "selector = SelectFromModel(estimator = selector_model, threshold = 'median')\n",
    "\n",
    "# Entrenamos el selector\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# Obtenemos los nombres de las columnas seleccionadas\n",
    "selected_features_mask = selector.get_support()\n",
    "selected_features_names = X_train.columns[selected_features_mask]\n",
    "\n",
    "# Transformamos nuestros conjuntos para quedarnos solo con las columnas seleccionadas\n",
    "X_train_selected = selector.transform(X_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# Reportamos los resultados\n",
    "print(\"\\nProceso de selección completado.\")\n",
    "print(\"Número original de características:\", X_train.shape[1])\n",
    "print(\"Número de características seleccionadas:\", X_train_selected.shape[1])\n",
    "print(\"\\nCaracterísticas seleccionadas:\")\n",
    "print(selected_features_names.tolist())\n",
    "\n",
    "# Convertimos los arrays de numpy de vuelta a DataFrames de Pandas\n",
    "X_train_selected = pd.DataFrame(X_train_selected, columns = selected_features_names)\n",
    "X_test_selected = pd.DataFrame(X_test_selected, columns = selected_features_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e43255d",
   "metadata": {},
   "source": [
    "## 6. Escalado de Características Numéricas (Scaling)\n",
    "\n",
    "Finalmente, escalamos los datos para que todas las características tengan una media de 0 y una desviación estándar de 1. Esto es crucial para el rendimiento de muchos modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "207d1096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el escalador\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# \"Ajustamos\" el escalador SÓLO con los datos de entrenamiento para que aprenda la media y desviación estándar\n",
    "scaler.fit(X_train_selected)\n",
    "\n",
    "# \"Transformamos\" tanto el conjunto de entrenamiento como el de prueba\n",
    "X_train_scaled = scaler.transform(X_train_selected)\n",
    "X_test_scaled = scaler.transform(X_test_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d775fdba",
   "metadata": {},
   "source": [
    "## Conclusión\n",
    "\n",
    "En este punto, tenemos nuestros datos listos para el modelado:\n",
    "- **X_train_scaled** y **y_train** para entrenar nuestros modelos.\n",
    "- **X_test_scaled** y **y_test** para evaluarlos de forma honesta."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Cyber-Project)",
   "language": "python",
   "name": "cyber-project-py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
