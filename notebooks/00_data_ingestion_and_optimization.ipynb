{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0227f69",
   "metadata": {},
   "source": [
    "# 00 - Ingesta y Optimización de Datos del CIC-IDS-2017\n",
    "\n",
    "**Objetivo:** Este notebook se encarga de leer los 8 archivos CSV originales del dataset, unirlos, realizar una limpieza inicial y guardarlos en un único archivo Parquet optimizado para su uso en análisis posteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ad00bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dccf71",
   "metadata": {},
   "source": [
    "## 1. Carga y Unión de Archivos CSV\n",
    "\n",
    "En esta sección, se localizan todos los archivos **.csv** dentro de la carpeta de datos crudos. Cada archivo se carga en un DataFrame de Pandas y luego se concatenan todos en un único DataFrame para facilitar su manejo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c22ef4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones del dataframe completo: (2830743, 79)\n"
     ]
    }
   ],
   "source": [
    "# Cargar y unir los archivos CSV\n",
    "# Definir la ruta donde están los archivos CSV\n",
    "path_to_csvs = r'../data/raw/DatasetCICIDS2017/MachineLearningCVE/'  # Cambia esta ruta según la ubicación de tus archivos CSV\n",
    "\n",
    "# Se utiliza glob para encontrar todos los archivos CSV en el directorio\n",
    "csv_files = glob.glob(os.path.join(path_to_csvs, \"*.csv\")) \n",
    "\n",
    "# Leer cada csv y guardarlo en una lista de dataframes\n",
    "df_list = [] # Lista para almacenar los dataframes\n",
    "for file_name in csv_files: # Iterar sobre cada archivo CSV\n",
    "    df = pd.read_csv(file_name) # Leer el archivo CSV\n",
    "    df_list.append(df) # Añadir el dataframe a la lista\n",
    "\n",
    "# Concatenar todos los dataframes en uno solo\n",
    "data = pd.concat(df_list, ignore_index = True)\n",
    "print(f\"Dimensiones del dataframe completo: {data.shape}\") # Imprimir las dimensiones del dataframe completo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587997b9",
   "metadata": {},
   "source": [
    "## 2. Limpieza Inicial\n",
    "\n",
    "Esta fase prepara el DataFrame para el análisis:\n",
    "1.  **Limpieza de Nombres:** Se eliminan los espacios en blanco de los nombres de las columnas.\n",
    "2.  **Manejo de Nulos:** Se reemplazan los valores infinitos (artefactos comunes en este dataset) por **NaN** y luego se eliminan las filas correspondientes.\n",
    "3.  **Manejo de caracteres de reemplazo:** Se sustituyen los caracteres de reemplazo por un espacio vacío."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1aa7bd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar espacios en los nombres de las columnas\n",
    "data.columns = data.columns.str.strip()\n",
    "\n",
    "# Reemplazamos infinitos con NaN\n",
    "data.replace([np.inf, -np.inf], np.nan, inplace = True)\n",
    "\n",
    "# Eliminamos las filas que contengan cualquier valor NaN\n",
    "data.dropna(inplace = True)\n",
    "\n",
    "# Limpiamos los caracteres de reemplazo de la columna 'Label'\n",
    "data['Label'] = data['Label'].str.replace('�', ' ', regex = False)\n",
    "data['Label'] = data['Label'].str.replace(r'\\s+', ' ', regex = True)\n",
    "data['Label'] = data['Label'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104c61e0",
   "metadata": {},
   "source": [
    "## 3. Optimización Inicial y Guardado en Formato Parquet\n",
    "\n",
    "Finalmente, se aplica una función para reducir drásticamente el uso de memoria del DataFrame limpio, convirtiendo las columnas numéricas al tipo de dato más pequeño posible (**downcasting**) y las columnas de texto a un tipo **category** eficiente. El DataFrame optimizado se guarda en formato **Parquet**. Este formato es columnar, comprimido y mucho más rápido de leer que el CSV, lo que agilizará todos los análisis futuros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcba25ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memoria usada por el DataFrame: 1726.00 MB\n",
      "Uso de memoria final: 655.34 MB\n",
      "Reducción del 62.0%\n"
     ]
    }
   ],
   "source": [
    "# Optimización de Memoria y guardado final\n",
    "def reduce_memory_usage(data):\n",
    "    start_mem = data.memory_usage().sum() / 1024**2 # Memoria inicial en MB\n",
    "    print(f\"Memoria usada por el DataFrame: {start_mem:.2f} MB\") \n",
    "\n",
    "    for col in data.columns: # Iterar sobre cada columna del DataFrame\n",
    "        col_type = data[col].dtype # Tipo de dato de la columna\n",
    "\n",
    "        if col_type != object and col_type.name != \"category\": # Si la columna no es de tipo object (cadena de texto)\n",
    "            c_min = data[col].min() # Valor mínimo de la columna\n",
    "            c_max = data[col].max() # Valor máximo de la columna\n",
    "\n",
    "            if str(col_type)[:3] == \"int\": # Si la columna es de tipo entero\n",
    "                if c_min >= np.iinfo(np.int8).min and c_max <= np.iinfo(np.int8).max: # Verificar si cabe en int8\n",
    "                    data[col] = data[col].astype(np.int8) # Convertir a int8\n",
    "                elif c_min >= np.iinfo(np.int16).min and c_max <= np.iinfo(np.int16).max: # Verificar si cabe en int16\n",
    "                    data[col] = data[col].astype(np.int16) # Convertir a int16\n",
    "                elif c_min >= np.iinfo(np.int32).min and c_max <= np.iinfo(np.int32).max: # Verificar si cabe en int32\n",
    "                    data[col] = data[col].astype(np.int32) # Convertir a int32\n",
    "                elif c_min >= np.iinfo(np.int64).min and c_max <= np.iinfo(np.int64).max: # Verificar si cabe en int64\n",
    "                    data[col] = data[col].astype(np.int64) # Convertir a int64\n",
    "            else:\n",
    "                if c_min >= np.finfo(np.float16).min and c_max <= np.finfo(np.float16).max: # Verificar si cabe en float16\n",
    "                    data[col] = data[col].astype(np.float16) # Convertir a float16\n",
    "                elif c_min >= np.finfo(np.float32).min and c_max <= np.finfo(np.float32).max: # Verificar si cabe en float32\n",
    "                    data[col] = data[col].astype(np.float32) # Convertir a float32\n",
    "                else:\n",
    "                    data[col] = data[col].astype(np.float64) # Convertir a float64\n",
    "        else:\n",
    "            data[col] = data[col].astype(\"category\") # Convertir columnas de tipo object a category\n",
    "\n",
    "    end_mem = data.memory_usage().sum() / 1024**2\n",
    "    print(f'Uso de memoria final: {end_mem:.2f} MB')\n",
    "    print(f'Reducción del {100 * (start_mem - end_mem) / start_mem:.1f}%')\n",
    "    return data\n",
    "\n",
    "# Llamamos a la función\n",
    "data = reduce_memory_usage(data)\n",
    "\n",
    "# Guardamos el DataFrame optimizado en formato Parquet\n",
    "path_to_save = r'../data/processed/cic_ids_2017_optimized.parquet'\n",
    "data.to_parquet(path_to_save)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Cyber-Project)",
   "language": "python",
   "name": "cyber-project-py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
