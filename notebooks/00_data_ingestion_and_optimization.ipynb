{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0227f69",
   "metadata": {},
   "source": [
    "# 00 - Ingesta y Optimización de Datos del CIC-IDS-2017\n",
    "\n",
    "**Objetivo:** Este notebook implementa el pipeline de ingesta de datos. Lee los 8 archivos CSV originales del dataset, los une, realiza una limpieza fundamental y guarda el resultado en un único archivo Parquet optimizado para su uso en análisis posteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ad00bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dccf71",
   "metadata": {},
   "source": [
    "## 1. Carga y Unión de Archivos CSV\n",
    "\n",
    "Se localizan todos los archivos `.csv` en el directorio de datos crudos y se concatenan en un solo DataFrame de Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c22ef4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset combinado cargado. Dimensiones: (2830743, 79)\n"
     ]
    }
   ],
   "source": [
    "# Definir la ruta a los datos crudos\n",
    "path_to_csvs = r'../data/raw/DatasetCICIDS2017/MachineLearningCVE/'\n",
    "\n",
    "# Encontrar todas las rutas de los archivos CSV\n",
    "csv_files = glob.glob(os.path.join(path_to_csvs, \"*.csv\"))\n",
    "\n",
    "# Cargar todos los archivos CSV en una lista de DataFrames usando una list comprehension\n",
    "df_list = [pd.read_csv(f) for f in csv_files]\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "df = pd.concat(df_list, ignore_index = True)\n",
    "\n",
    "# Imprimir las dimensiones finales como confirmación\n",
    "print(f\"Dataset combinado cargado. Dimensiones: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587997b9",
   "metadata": {},
   "source": [
    "## 2. Limpieza y Optimización\n",
    "\n",
    "Esta fase prepara el DataFrame para el análisis. Incluye los siguientes sub-pasos:\n",
    "1.  **Limpieza de Nombres de Columnas:** Se eliminan los espacios en blanco.\n",
    "2.  **Manejo de Valores Inválidos:** Se reemplazan los valores infinitos por `NaN` y se eliminan las filas correspondientes.\n",
    "3.  **Limpieza de Etiquetas:** Se corrigen caracteres mal codificados en la columna `Label`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1aa7bd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar la secuencia de limpieza de datos\n",
    "df.columns = df.columns.str.strip()\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace = True)\n",
    "df.dropna(inplace = True)\n",
    "\n",
    "# Limpiar la columna 'Label' encadenando los métodos de string\n",
    "df['Label'] = (df['Label']\n",
    "               .str.replace('�', ' ', regex = False)\n",
    "               .str.replace(r'\\s+', ' ', regex = True)\n",
    "               .str.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104c61e0",
   "metadata": {},
   "source": [
    "## 3. Optimización Inicial y Guardado en Formato Parquet\n",
    "\n",
    "Finalmente, se aplica una función para reducir drásticamente el uso de memoria del DataFrame limpio, convirtiendo las columnas numéricas al tipo de dato más pequeño posible (`downcasting`) y las columnas de texto a un tipo `category` eficiente. El DataFrame optimizado se guarda en formato `Parquet`. Este formato es columnar, comprimido y mucho más rápido de leer que el `.csv`, lo que agilizará todos los análisis futuros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcba25ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uso de memoria inicial: 1726.00 MB\n",
      "Uso de memoria final: 655.34 MB\n",
      "Reducción del 62.0%\n",
      "\n",
      "DataFrame optimizado y guardado exitosamente en: ../data/processed/cic_ids_2017_optimized.parquet\n"
     ]
    }
   ],
   "source": [
    "def optimize_memory(df):\n",
    "    \"\"\"\n",
    "    Itera sobre todas las columnas de un DataFrame y modifica los tipos de datos\n",
    "    para reducir el uso de memoria, aplicando 'downcasting' a tipos numéricos\n",
    "    y convirtiendo tipos 'object' a 'category'.\n",
    "\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f'Uso de memoria inicial: {start_mem:.2f} MB')\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object and col_type.name != 'category':\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min >= np.iinfo(np.int8).min and c_max <= np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min >= np.iinfo(np.int16).min and c_max <= np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min >= np.iinfo(np.int32).min and c_max <= np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min >= np.iinfo(np.int64).min and c_max <= np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min >= np.finfo(np.float16).min and c_max <= np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min >= np.finfo(np.float32).min and c_max <= np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        elif col_type == 'object':\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f'Uso de memoria final: {end_mem:.2f} MB')\n",
    "    print(f'Reducción del {100 * (start_mem - end_mem) / start_mem:.1f}%')\n",
    "    return df\n",
    "\n",
    "# Aplicar la función de optimización de memoria\n",
    "df = optimize_memory(df)\n",
    "\n",
    "# Guardar el DataFrame final en formato Parquet\n",
    "path_to_save = '../data/processed/cic_ids_2017_optimized.parquet'\n",
    "df.to_parquet(path_to_save)\n",
    "\n",
    "print(f\"\\nDataFrame optimizado y guardado exitosamente en: {path_to_save}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Cyber-Project)",
   "language": "python",
   "name": "cyber-project-py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
